# Lexers and Unicode text
# 词法分析器与 Unicode 文本

[原文链接](https://github.com/antlr/antlr4/blob/master/doc/unicode.md)

Prior to ANTLR 4.7, generated lexers in most targets only supported part of the Unicode standard (code points up to `U+FFFF`). As of ANTLR 4.7, the lexers in all language runtimes support the full range of Unicode code points up to `U+10FFFF`.
在 ANTLR 4.7 之前，大多数目标生成的词法分析器仅支持部分 Unicode 标准（码点至 `U+FFFF`）。从 ANTLR 4.7 开始，所有语言运行时的词法分析器都支持完整的 Unicode 码点范围，直至 `U+10FFFF`。

C++, Python, Go, and Swift APIs didn't need any API changes to support Unicode code points, so we decided to leave those class interfaces as-is.
C++、Python、Go 和 Swift API 不需要任何 API 更改来支持 Unicode 码点，因此我们决定保持这些类接口不变。

Java, C#, and JavaScript runtimes required changes and, rather than break the previous interface, we deprecated them. (The *Java-target* deprecated `ANTLRInputStream` and `ANTLRFileStream` APIs only support Unicode code points up to `U+FFFF`.) Now, those targets must create `CharStream`s from input using `CharStreams.fromPath()`, `CharStreams.fromFileName()`, etc...
Java、C# 和 JavaScript 运行时需要进行更改，但为了避免破坏之前的接口，我们将其弃用。（*Java 目标*中弃用的 `ANTLRInputStream` 和 `ANTLRFileStream` API 仅支持 Unicode 码点至 `U+FFFF`。）现在，这些目标必须使用 `CharStreams.fromPath()`、`CharStreams.fromFileName()` 等方法从输入创建 `CharStream`。

A big shout out to Ben Hamilton (github bhamiltoncx) for his superhuman efforts across all targets to get true support for U+10FFFF code points.
特别感谢 Ben Hamilton (github bhamiltoncx) 在所有目标上付出的超人努力，以实现对 U+10FFFF 码点的真正支持。

## Example
## 示例

The Java, C#, and JavaScript runtimes use the new factory style stream creation interface. For example, here is some sample Java code that uses `CharStreams.fromPath()`:
Java、C# 和 JavaScript 运行时使用新的工厂式流创建接口。例如，以下是一些使用 `CharStreams.fromPath()` 的 Java 示例代码：

```java
public static void main(String[] args) {
  CharStream charStream = CharStreams.fromPath(Paths.get(args[0]));
  Lexer lexer = new UnicodeLexer(charStream);
  CommonTokenStream tokens = new CommonTokenStream(lexer);
  tokens.fill();
  for (Token token : tokens.getTokens()) {
    System.out.println("Got token: " + token.toString());
  }
}
```

# Unicode Code Points in Lexer Grammars
# 词法语法中的 Unicode 码点

To refer to Unicode [code points](https://en.wikipedia.org/wiki/Code_point) in lexer grammars, use the `\u` string escape plus up to 4 hex digits. For example, to create a lexer rule for a single Cyrillic character by creating a range from `U+0400` to `U+04FF`:
要在词法语法中引用 Unicode [码点](https://en.wikipedia.org/wiki/Code_point)，请使用 `\u` 字符串转义加上最多 4 个十六进制数字。例如，要通过创建从 `U+0400` 到 `U+04FF` 的范围来为单个西里尔字母创建词法规则：

```ANTLR
CYRILLIC : '\u0400'..'\u04FF' ; // or [\u0400-\u04FF] without quotes
```

Unicode literals larger than U+FFFF must use the extended `\u{12345}` syntax. For example, to create a lexer rule for a selection of smiley faces from the [Emoticons Unicode block](http://www.unicode.org/charts/PDF/U1F600.pdf):
大于 U+FFFF 的 Unicode 字面量必须使用扩展的 `\u{12345}` 语法。例如，要为 [Emoticons Unicode 块](http://www.unicode.org/charts/PDF/U1F600.pdf) 中的一组笑脸创建词法规则：

```ANTLR
EMOTICONS : ('\u{1F600}' | '\u{1F602}' | '\u{1F615}') ; // or [\u{1F600}\u{1F602}\u{1F615}]
```

Finally, lexer char sets can include Unicode properties. Each Unicode code point has at least one property that describes the type group to which it belongs (e.g. alpha, number, punctuation). Other properties can be the language script or special binary properties and Unicode code blocks. That means however, that a property specifies a group of code points, hence they are only allowed in lexer char sets.
最后，词法字符集可以包含 Unicode 属性。每个 Unicode 码点至少有一个描述其所属类型组（例如字母、数字、标点符号）的属性。其他属性可以是语言文字或特殊的二进制属性以及 Unicode 码块。然而，这意味着属性指定了一组码点，因此它们只允许在词法字符集中使用。

```ANTLR
EMOJI : [\p{Emoji}] ;
JAPANESE : [\p{Script=Hiragana}\p{Script=Katakana}\p{Script=Han}] ;
NOT_CYRILLIC : [\P{Script=Cyrillic}] ;
```

See [lexer-rules.md](lexer-rules.md#lexer-rule-elements) for more detail on Unicode escapes in lexer rules.
有关词法规则中 Unicode 转义的更多详细信息，请参阅 [lexer-rules.md](lexer-rules.md#lexer-rule-elements)。

## Migration
## 迁移

Code for **4.6** looked like this:
**4.6** 版本的代码看起来像这样：

```java
CharStream input = new ANTLRFileStream("myinputfile");
JavaLexer lexer = new JavaLexer(input);
CommonTokenStream tokens = new CommonTokenStream(lexer);
```

(It didn't use UTF-8 by default, despite the documentation saying so previously; it actually depended on the calling environments default.)
（尽管之前的文档这么说，但它默认并不使用 UTF-8；实际上它依赖于调用环境的默认设置。）

Code for **4.7** assumes UTF-8 by default and looks like this:
**4.7** 版本默认假定使用 UTF-8，代码看起来像这样：

```java
CharStream input = CharStreams.fromFileName("inputfile");
JavaLexer lexer = new JavaLexer(input);
CommonTokenStream tokens = new CommonTokenStream(lexer);
```

Or, if you'd like to specify the file encoding:
或者，如果您想指定文件编码：

```java
CharStream input = CharStreams.fromFileName("inputfile", Charset.forName("windows-1252"));
```

### Motivation
### 动机

After a [lively discussion](https://github.com/antlr/antlr4/pull/1771), I (parrt) decided not to simply gut the 4.6 `ANTLRFileStream` and `ANTLRInputStream` to incorporate the new U+10FFFF functionality. I decided to *deprecate* the old interface and recommend use of the new interface to prevent confusion. My reasoning is summarized as:
经过[热烈的讨论](https://github.com/antlr/antlr4/pull/1771)，我（parrt）决定不简单地修改 4.6 的 `ANTLRFileStream` 和 `ANTLRInputStream` 来整合新的 U+10FFFF 功能。我决定*弃用*旧接口并推荐使用新接口，以防止混淆。我的理由总结如下：

* I didn't like the idea of breaking all 4.6 code. To get the previous streams to properly support > 16 bit Unicode would require a lot of changes to the method signatures.
* 我不喜欢破坏所有 4.6 代码的想法。要让之前的流正确支持 > 16 位的 Unicode，需要对方法签名进行大量更改。
* Using `int` buffer element types would double the size of memory required to hold streams in memory, given that we buffer everything (and I didn't want to change that aspect of the streams).
* 使用 `int` 缓冲区元素类型会使内存中保存流所需的内存大小加倍，因为我们缓冲所有内容（而且我不想改变流的这方面）。
* The new factory-style interface supports creation of the smallest possible code point buffer element size according to the Unicode code points found in the input stream. This means using half as much memory as the old {@link ANTLRFileStream}, which assumed 16-bit characters, for ASCII text.
* 新的工厂式接口支持根据输入流中找到的 Unicode 码点创建尽可能小的码点缓冲区元素大小。这意味着对于 ASCII 文本，使用的内存是旧 {@link ANTLRFileStream} 的一半，后者假定为 16 位字符。
* Through some [serious testing and performance tweaking](https://github.com/antlr/antlr4/pull/1781), the new streams perform as fast or faster than the 4.6 streams.
* 通过一些[认真的测试和性能调整](https://github.com/antlr/antlr4/pull/1781)，新流的性能与 4.6 流一样快或更快。

**WARNING**. *You should avoid using both the deprecated and the new streams* in the same application because you will see a nontrivial performance degradation. This speed hit is because the `Lexer`'s internal code goes from a monomorphic to megamorphic dynamic dispatch to get characters from the input stream. Java's on-the-fly compiler (JIT) is unable to perform the same optimizations so stick with either the old or the new streams, if performance is a primary concern. See the [extreme debugging and spelunking](https://github.com/antlr/antlr4/pull/1781) needed to identify this issue in our timing rig.
**警告**。*您应避免在同一应用程序中同时使用已弃用的流和新流*，因为您会看到明显的性能下降。这种速度损失是因为 `Lexer` 的内部代码从单态变为超多态动态分派以从输入流中获取字符。Java 的即时编译器 (JIT) 无法执行相同的优化，因此如果性能是主要关注点，请坚持使用旧流或新流。请参阅在我们的计时装置中识别此问题所需的[极端调试和探索](https://github.com/antlr/antlr4/pull/1781)。

### Legacy grammar using surrogate code units
### 使用代理码单元的旧版语法

Legacy grammars that did their own UTF-16 surrogate code unit matching will need to continue to use `ANTLRInputStream` (Java target) until the parser-application code can upgrade to `CharStreams` interface. Then the surrogate code unit matching should be removed from the grammar in favor of letting the new streams do the decoding.
执行自己的 UTF-16 代理码单元匹配的旧版语法需要继续使用 `ANTLRInputStream`（Java 目标），直到解析器应用程序代码可以升级到 `CharStreams` 接口。然后应从语法中移除代理码单元匹配，转而让新流进行解码。

Prior to 4.7, application code could directly pass `Token.getStartIndex()` and `Token.getStopIndex()` to Java and C# String APIs (because both used UTF-16 code units as the fundamental unit of length).  With the new streams, clients will have to convert from code point indices to UTF-16 code unit indices. Here is some (Java) code to show you the necessary logic:
在 4.7 之前，应用程序代码可以直接将 `Token.getStartIndex()` 和 `Token.getStopIndex()` 传递给 Java 和 C# String API（因为两者都使用 UTF-16 码元作为长度的基本单位）。对于新流，客户端必须将码点索引转换为 UTF-16 码元索引。以下是一些（Java）代码，向您展示必要的逻辑：

```java
public final class CodePointCounter {
  private final String input;
  public int inputIndex = 0;
  public int codePointIndex = 0;
  
  public int advanceToIndex(int newCodePointIndex) {
    assert newCodePointIndex >= codePointIndex;
    while (codePointIndex < newCodePointOffset) {
        int codePoint = Character.codePointAt(input, inputIndex);
        inputIndex += Character.charCount(codePoint);
        codePointIndex++;
    }
    return inputIndex;
  }
}
```

### Character Buffering, Unbuffered streams
### 字符缓冲，无缓冲流

The ANTLR character streams still buffer all the input when you create the stream, as they have done for ~20 years.
ANTLR 字符流在您创建流时仍然缓冲所有输入，正如它们过去约 20 年所做的那样。

If you need unbuffered access, please note that it becomes challenging to create parse trees. The parse tree has to point to tokens which will either point into a stale location in an unbuffered stream or you have to copy the characters out of the buffer into the token. That defeats the purpose of unbuffered input. See the [ANTLR 4 book](https://www.amazon.com/Definitive-ANTLR-4-Reference/dp/1934356999) "13.8 Unbuffered Character and Token Streams". Unbuffered streams are primarily useful for processing infinite streams *during the parse* and require that you manually buffer characters. Use `UnbufferedCharStream` and `UnbufferedTokenStream`.
如果您需要无缓冲访问，请注意创建解析树会变得具有挑战性。解析树必须指向词法符号，这些词法符号要么指向无缓冲流中的过时位置，要么您必须将字符从缓冲区复制到词法符号中。这违背了无缓冲输入的目的。请参阅 [ANTLR 4 书籍](https://www.amazon.com/Definitive-ANTLR-4-Reference/dp/1934356999) "13.8 无缓冲字符和词法符号流"。无缓冲流主要用于*在解析期间*处理无限流，并且需要您手动缓冲字符。使用 `UnbufferedCharStream` 和 `UnbufferedTokenStream`。

```java
CharStream input = new UnbufferedCharStream(is);
CSVLexer lex = new CSVLexer(input); // copy text out of sliding buffer and store in tokens
lex.setTokenFactory(new CommonTokenFactory(true));
TokenStream tokens = new UnbufferedTokenStream<CommonToken>(lex);
CSVParser parser = new CSVParser(tokens);
parser.setBuildParseTree(false);
parser.file();
```

Your grammar that needs to have embedded actions that access the tokens as they are created, but before they disappear and are garbage collected. For example,
您的语法需要具有嵌入式动作，这些动作在词法符号创建时但在它们消失并被垃圾回收之前访问它们。例如，

```
data : a=INT {int x = Integer.parseInt($a.text);} ;
```

From the code comments of `CommonTokenFactory`:
来自 `CommonTokenFactory` 的代码注释：

> That `true` in `new CommonTokenFactory(true)` indicates whether `CommonToken.setText` should be called after constructing tokens to explicitly set the text. This is useful for cases where the input stream might not be able to provide arbitrary substrings of text from the input after the lexer creates a token (e.g. the implementation of `CharStream.getText` in `UnbufferedCharStream` throws an `UnsupportedOperationException`). Explicitly setting the token text allows `Token.getText` to be called at any time regardless of the input stream implementation.
> `new CommonTokenFactory(true)` 中的 `true` 表示是否应在构造词法符号后调用 `CommonToken.setText` 以显式设置文本。这对于以下情况很有用：在词法分析器创建词法符号后，输入流可能无法提供输入中的任意文本子字符串（例如，`UnbufferedCharStream` 中的 `CharStream.getText` 实现会抛出 `UnsupportedOperationException`）。显式设置词法符号文本允许在任何时候调用 `Token.getText`，而不管输入流的实现如何。

*Currently, only Java, C++, and C# have these unbuffered streams implemented*.
*目前，只有 Java、C++ 和 C# 实现了这些无缓冲流*。
