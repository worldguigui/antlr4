# Wildcard Operator and Nongreedy Subrules
# 通配符运算符和非贪婪子规则

[原文链接](https://github.com/antlr/antlr4/blob/master/doc/wildcard.md)

EBNF subrules like `(...)?`, `(...)*` and `(...)+` are greedy—They consume as much input as possible, but sometimes that’s not what’s needed. Constructs like `.*` consume until the end of the input in the lexer and sometimes in the parser. We want that loop to be nongreedy so we need to use different syntax: `.*?` borrowed from regular expression notation. We can make any subrule that has a `?`, `*`, or `+` suffix nongreedy by adding another `?` suffix. Such nongreedy subrules are allowed in both the parser and the lexer, but they are used much more frequently in the lexer.
EBNF 子规则如 `(...)?`、`(...)*` 和 `(...)+` 是贪婪的——它们会消耗尽可能多的输入，但有时这并不是我们需要的。像 `.*` 这样的结构在词法分析器中会一直消耗到输入结束，有时在解析器中也是如此。我们希望这个循环是非贪婪的，所以需要使用不同的语法：借用正则表达式表示法的 `.*?`。我们可以通过添加另一个 `?` 后缀来使任何具有 `?`、`*` 或 `+` 后缀的子规则变为非贪婪。这种非贪婪子规则在解析器和词法分析器中都允许使用，但在词法分析器中使用得更频繁。

## Nongreedy Lexer Subrules
## 非贪婪词法子规则

Here’s the very common C-style comment lexer rule that consumes any characters until it sees the trailing `*/`:
这是一个非常常见的 C 风格注释词法规则，它会消耗任何字符，直到看到结束的 `*/`：

```
COMMENT : '/*' .*? '*/' -> skip ; // .*? matches anything until the first */
```

Here’s another example that matches strings that allow \" as an escaped quote character:
这是另一个例子，匹配允许 \" 作为转义引号字符的字符串：

```
grammar Nongreedy;
s : STRING+ ;
STRING : '"' ( '\\"' | . )*? '"' ; // match "foo", "\"", "x\"\"y", ...
WS : [ \r\t\n]+ -> skip ;
```

```bash
$ antlr4 Nongreedy.g4
$ javac Nongreedy*.java
$ grun Nongreedy s -tokens
=> 	"quote:\""
=> 	EOF
<= 	[@0,0:9='"quote:\""',<1>,1:0]
 	[@1,11:10='<EOF>',<-1>,2:0]
```

Nongreedy subrules should be used sparingly because they complicate the recognition problem and sometimes make it tricky to decipher how the lexer will match text. Here is how the lexer chooses token rules:
非贪婪子规则应谨慎使用，因为它们使识别问题复杂化，有时会使理解词法分析器如何匹配文本变得棘手。以下是词法分析器选择词法符号规则的方式：

<ol>
<li>The primary goal is to match the lexer rule that recognizes the most input characters.
主要目标是匹配识别最多输入字符的词法规则。

```
INT : [0-9]+ ;
DOT : '.' ; // match period
FLOAT : [0-9]+ '.' ; // match FLOAT upon '34.' not INT then DOT
```
</li>
<li>
If more than one lexer rule matches the same input sequence, the priority goes to the rule occurring first in the grammar file.
如果多个词法规则匹配相同的输入序列，则优先级给予语法文件中首先出现的规则。

```
DOC : '/**' .*? '*/' ; // both rules match /** foo */, resolve to DOC
CMT : '/*' .*? '*/' ;
```
</li>
<li>
Nongreedy subrules match the fewest number of characters that still allows the surrounding lexical rule to match.
非贪婪子规则匹配最少数量的字符，同时仍允许周围的词法规则匹配。

```
/** Match anything except \n inside of double angle brackets */
STRING : '<<' ~'\n'*? '>>' ; // Input '<<foo>>>>' matches STRING then END
END    : '>>' ;
```
</li>
<li>
<p>After crossing through a nongreedy subrule within a lexical rule, all decision-making from then on is "first match wins."
在穿过词法规则中的非贪婪子规则后，所有后续决策都是"首次匹配获胜"。
</p>
<p>
For example, literal `ab` in rule right-hand side (grammar fragment) `.*? ('a'|'ab')` is dead code and can never be matched. If the input is ab, the first alternative, 'a', matches the first character and therefore succeeds. ('a'|'ab') by itself on the right-hand side of a rule properly matches the second alternative for input ab. This quirk arises from a nongreedy design decision that’s too complicated to go into here.</p>
例如，规则右侧（语法片段）`.*? ('a'|'ab')` 中的字面量 `ab` 是死代码，永远无法匹配。如果输入是 ab，第一个选项 'a' 匹配第一个字符，因此成功。单独在规则右侧的 ('a'|'ab') 会正确匹配输入 ab 的第二个选项。这个怪癖源于一个过于复杂而无法在此详述的非贪婪设计决策。
<li>
</ol>

To illustrate the different ways to use loops within lexer rules, consider the following grammar, which has three different action-like tokens (using different delimiters so that they all fit within one example grammar).
为了说明在词法规则中使用循环的不同方式，请考虑以下语法，它具有三个不同的类似动作的词法符号（使用不同的分隔符，以便它们都能容纳在一个示例语法中）。

```
ACTION1 : '{' ( STRING | . )*? '}' ; // Allows {"foo}
ACTION2 : '[' ( STRING | ~'"' )*? ']' ; // Doesn't allow ["foo]; nongreedy *?
ACTION3 : '<' ( STRING | ~[">] )* '>' ; // Doesn't allow <"foo>; greedy *
STRING : '"' ( '\\"' | . )*? '"' ;
```

Rule `ACTION1` allows unterminated strings, such as `"foo`, because input `"foo` matches to the wildcard part of the loop. It doesn’t have to go into rule `STRING` to match a quote. To fix that, rule `ACTION2` uses `~'"'` to match any character but the quote. Expression `~'"'` is still ambiguous with the `']'` that ends the rule, but the fact that the subrule is nongreedy means that the lexer will exit the loop upon a right square bracket. To avoid a nongreedy subrule, make the alternatives explicit. Expression `~[">]` matches anything but the quote and right angle bracket. Here’s a sample run:
规则 `ACTION1` 允许未终止的字符串，例如 `"foo`，因为输入 `"foo` 匹配循环的通配符部分。它不必进入规则 `STRING` 来匹配引号。为了解决这个问题，规则 `ACTION2` 使用 `~'"'` 来匹配除引号之外的任何字符。表达式 `~'"'` 与结束规则的 `']'` 仍然存在歧义，但子规则是非贪婪的这一事实意味着词法分析器会在遇到右方括号时退出循环。为了避免非贪婪子规则，可以明确指定替代项。表达式 `~[">]` 匹配除引号和右尖括号之外的任何内容。以下是一个示例运行：

```bash
$ antlr4 Actions.g4
$ javac Actions*.java
$ grun Actions tokens -tokens
=> 	{"foo}
=> 	EOF
<= 	[@0,0:5='{"foo}',<1>,1:0]
 	[@1,7:6='<EOF>',<-1>,2:0]
=> 	$ grun Actions tokens -tokens
=> 	["foo]
=> 	EOF
<= 	line 1:0 token recognition error at: '["foo]
 	'
 	[@0,7:6='<EOF>',<-1>,2:0]
=> 	$ grun Actions tokens -tokens
=> 	<"foo>
=> 	EOF
<= 	line 1:0 token recognition error at: '<"foo>
 	'
 	[@0,7:6='<EOF>',<-1>,2:0]
```

## Nongreedy Parser Subrules
## 非贪婪解析器子规则

Nongreedy subrules and wildcard are also useful within parsers to do *fuzzy parsing* where the goal is to extract information from an input file without having to specify the full grammar. In contrast to nongreedy lexer decision-making, parsers always make globally correct decisions. A parser never makes a decision that will ultimately cause valid input to fail later on during the parse. Here is the central idea: Nongreedy parser subrules match the shortest sequence of tokens that preserves a successful parse for a valid input sentence.
非贪婪子规则和通配符在解析器中也很有用，用于进行*模糊解析*，其目标是从输入文件中提取信息，而无需指定完整的语法。与非贪婪词法分析器决策相反，解析器总是做出全局正确的决策。解析器永远不会做出最终导致有效输入在后续解析过程中失败的决策。核心思想是：非贪婪解析器子规则匹配最短的词法符号序列，该序列能够保持对有效输入句子的成功解析。

For example, here are the key rules that demonstrate how to pull integer constants out of an arbitrary Java file:
例如，以下是演示如何从任意 Java 文件中提取整数常量的关键规则：

```
grammar FuzzyJava;
 
/** Match anything in between constant rule matches */
file : .*? (constant .*?)+ ;
 
/** Faster alternate version (Gets an ANTLR tool warning about
 * a subrule like .* in parser that you can ignore.)
 */
altfile : (constant | .)* ; // match a constant or any token, 0-or-more times

/** Match things like "public static final SIZE" followed by anything */
constant
    :   'public' 'static' 'final' 'int' Identifier
        {System.out.println("constant: "+$Identifier.text);}
    ;
 
Identifier : [a-zA-Z_$] [a-zA-Z_$0-9]* ; // simplified
```

The grammar contains a greatly simplified set of lexer rules from a real Java lexer; the whole file about 60 lines. The recognizer still needs to handle string and character constants as well as comments so it doesn’t get out of sync, trying to match a constant inside of the string for example. The only unusual lexer rule performs “match any character not matched by another lexer rule” functionality:
该语法包含从真实 Java 词法分析器中大大简化的词法规则集；整个文件大约 60 行。识别器仍然需要处理字符串和字符常量以及注释，以免失去同步，例如尝试在字符串内部匹配常量。唯一不寻常的词法规则执行"匹配任何未被其他词法规则匹配的字符"的功能：

```
OTHER : . -> skip ;
```

This catchall lexer rule and the `.*?` subrule in the parser are the critical ingredients for fuzzy parsing.
这个全能词法规则和解析器中的 `.*?` 子规则是模糊解析的关键要素。

Here’s a sample file that we can run into the fuzzy parser:
以下是一个我们可以运行到模糊解析器中的示例文件：

```java
import java.util.*;
public class C {
    public static final int A = 1;
    public static final int B = 1;
    public void foo() { }
    public static final int C = 1;
}
```

And here’s the build and test sequence:
以下是构建和测试序列：

```bash
$ antlr4 FuzzyJava.g4
$ javac FuzzyJava*.java
$ grun FuzzyJava file C.java
constant: A
constant: B
constant: C
```

Notice that it totally ignores everything except for the `public static final int` declarations. This all happens with only two parser rules.
请注意，它完全忽略了除 `public static final int` 声明之外的所有内容。这一切仅用两个解析器规则就完成了。

Now let's try matching some simple class defs w/o having to build parser rules for the junk inside.  Here want to catch just `A` and `B`:
现在让我们尝试匹配一些简单的类定义，而不必为其中的垃圾内容构建解析器规则。这里只想捕获 `A` 和 `B`：

```
class A {
        String name = "parrt";
}

class B {
        int x;   
        int getDubX() {
                return 2*x;
        }
}
```

This grammar does it.
这个语法可以做到。

```
grammar Island;
file : clazz* ;
clazz : 'class' ID '{' ignore '}' ;
ignore : (method|.)*? ;
method : type ID '()' block ;
type : 'int' | 'void' ;
block : '{' (block | .)*? '}' ;
ID : [a-zA-Z] [a-zA-Z0-9]* ;
WS : [ \r\t\n]+ -> skip ;
ANY : . ;
```

You get:
你得到：

<img src=images/nonnested-fuzzy.png width=450>

Now let's try some nested classes
现在让我们尝试一些嵌套类

```
class A {
        String name = "parrt";
        class Nested {
            any filthy shite we want in here { }}}}}}
        }
}

class B {
        int x;   
        int getDubX() {
                return 2*x;
        }
}

```

```
grammar Island;
file : clazz* ;
clazz : 'class' ID '{' ignore '}' ;
ignore : (method|clazz|.)*? ; // <- only change is to add clazz alt here
method : type ID '()' block ;
type : 'int' | 'void' ;
block : '{' (block | .)*? '}' ;
ID : [a-zA-Z] [a-zA-Z0-9]* ;
WS : [ \r\t\n]+ -> skip ;
ANY : . ;
```

You get:
你得到：

<img src=images/nested-fuzzy.png width=600>
